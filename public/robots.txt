# Generic robots.txt for a website

# Allow all search engines to crawl the entire site
User-agent: *
Disallow:

# Other common directives:

# Block access to sensitive or private directories
Disallow: /admin/
Disallow: /api/
Disallow: /api/NewsApi

# Block access to specific files or directories
Disallow: /cgi-bin/
Disallow: /tmp/

# Allow access to certain directories (even if other rules block others)
Allow: /public/

# Prevent crawling of dynamic URL parameters
Disallow: /*?sessionid

# Sitemap - Specify the location of your sitemap XML
Sitemap: https://cavendish.vercel.app/sitemap.xml
